// app/_ai/types/llm.types.ts

// -----------------------------------------------------------------
// 1. é…ç½®èˆ‡è«‹æ±‚ä»‹é¢
// -----------------------------------------------------------------

export interface LLMProviderConfig {
  apiKey: string;
  baseURL?: string;
  model: string;
  temperature?: number;
  maxTokens?: number;
}

export interface LLMRequest {
  prompt: string;
  systemPrompt?: string;
  temperature?: number;
  maxTokens?: number;
  // æ–°å¢ž JSON ç›¸é—œé¸é … (å·²ä¿ç•™)
  responseMimeType?: 'text/plain' | 'application/json';
  responseSchema?: any; // JSON Schema ç‰©ä»¶
}

// -----------------------------------------------------------------
// 2. å›žæ‡‰èˆ‡å…ƒæ•¸æ“šä»‹é¢
// -----------------------------------------------------------------

/**
 * AI æœå‹™å¯¦éš›ä½¿ç”¨é‡æ•¸æ“š
 */
export interface LLMUsage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  // æœªä¾†å¯æ“´å±•ï¼š
  // completionTime?: number; 
  // totalCost?: number; 
}

/**
 * AI æœå‹™å›žæ‡‰çš„å…ƒæ•¸æ“š (Metadata)
 */
export interface LLMResponseMetadata {
    provider: 'gemini' | 'mock-simulator';
    model: string;
    latency: number;
    tokens: number; // ðŸ‘ˆ ç¢ºä¿åŠ å…¥é€™è¡Œï¼
    validated: boolean;
    timestamp: string;
    source: string;
    // ...
}

/**
 * åŽŸå§‹ LLM æœå‹™å›žæ‡‰
 */
export interface LLMResponse {
  content: string; // åŽŸå§‹æ–‡æœ¬æˆ– JSON å­—ä¸²
  model: string;
  latency: number;
  provider: string;
  usage?: LLMUsage;
}

// -----------------------------------------------------------------
// 3. Provider æŠ½è±¡ä»‹é¢
// -----------------------------------------------------------------

export interface LLMProvider {
  name: string;
  call: (request: LLMRequest) => Promise<LLMResponse>;
  validateConfig: (config: LLMProviderConfig) => boolean;
}